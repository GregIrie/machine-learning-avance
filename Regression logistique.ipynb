{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression logistique comme réseau de neurones\n",
    "\n",
    "Objectif : Construire une régression logistique à la manière dont on construit un réseau de neurones. \n",
    "\n",
    "**Instructions:**\n",
    "- Ne pas utiliser de boucles (for/while) à moins que ce ne soit explicitement demandé\n",
    "\n",
    "**On va apprendre à:**\n",
    "- Construire les briques principales d'un algorithme d'apprentissage:\n",
    "    - Initialisation des paramètres\n",
    "    - Calcul de la fonction coût et son gradient\n",
    "    - Appliquer une optimisation (ici descente de gradient) \n",
    "- Organiser les 3 briques ci-dessus dans un seul algo d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "Import des packages nécessaires\n",
    "- [numpy](www.numpy.org)\n",
    "- [h5py](http://www.h5py.org)\n",
    "- [matplotlib](http://matplotlib.org) \n",
    "- [PIL](http://www.pythonware.com/products/pil/)\n",
    "- [scipy](https://www.scipy.org/) \n",
    "- [pandas](https://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Présentation globale du problème\n",
    "\n",
    "Télécharger le data set au lien :\n",
    "\n",
    "http://www.robots.ox.ac.uk/~vgg/data/flowers/\n",
    "\n",
    "**Hypothèses**:\n",
    "Le dossier contient :\n",
    "    - des images en jpg de tailles différentes\n",
    "    - 17 catégories de fleurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la matrice des images \n",
    "def create_data(path):\n",
    "    \"\"\"\n",
    "    Crée un array contenant les valeurs RGB de chaque pixel et un array contenant les catégories\n",
    "    Argument:\n",
    "    - path chemin contenant le fichier téléchargé ci-dessus\n",
    "    Ouput:\n",
    "    - X coordonnées RGB de l'image resizée\n",
    "    - Y catégorie de l'image\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Images\n",
    "    new_width  = 64\n",
    "    new_height = 64\n",
    "    with open(path+'/files.txt') as f:\n",
    "        filenames=f.readlines()\n",
    "    n_files = 160\n",
    "    X = np.zeros((n_files,new_width, new_width, 3),dtype=int)\n",
    "    for i,f in enumerate(filenames):\n",
    "        if i in set(np.arange(881,960)):\n",
    "                img = Image.open(path+'/'+f.rstrip())\n",
    "                img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "                rgb = np.array(img).astype(int)\n",
    "                X[i-881,:,:,:] = rgb\n",
    "        if i in set(np.arange(1121,1200)):\n",
    "                img = Image.open(path+'/'+f.rstrip())\n",
    "                img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "                rgb = np.array(img).astype(int)\n",
    "                X[i-1121+80,:,:,:] = rgb\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "     \n",
    "    ### Labels\n",
    "    Y=np.zeros(X.shape[0],dtype=int)\n",
    "    #thresholds = np.arange(0,X.shape[0],80)\n",
    "    #for i,s in enumerate(thresholds[:-1]):\n",
    "    #    Y[s:thresholds[i+1]]=i\n",
    "    #Y[thresholds[-1]:]= i+1\n",
    "    Y[:80]=1\n",
    "    \n",
    "    sample = np.arange(80,X.shape[0])\n",
    "    np.random.shuffle(sample)\n",
    "    #X[80:160,:,:,:]=X[sample[:80],:,:,:]\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    \n",
    "    Y_train = Y_train.reshape(len(Y_train), 1)\n",
    "    Y_train = onehot_encoder.fit_transform(Y_train).astype(int)\n",
    "    Y_train = Y_train[:,1].reshape(1,Y_train.shape[0])\n",
    "    \n",
    "    Y_test = Y_test.reshape(len(Y_test), 1)\n",
    "    Y_test = onehot_encoder.fit_transform(Y_test).astype(int)\n",
    "    Y_test = Y_test[:,1].reshape(1,Y_test.shape[0])\n",
    "\n",
    "    classes = np.array([b'non buttercup', b'buttercup'], \n",
    "      dtype='|S13')\n",
    "    return(X_train, X_test, Y_train, Y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, classes = create_data('flower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'image\n",
    "index = 100\n",
    "plt.imshow(X_train[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train[:,index])) + \" c'est une image de \" + classes[int(np.squeeze(Y_train[:,index]))].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'on construit une architecture deep learning, on peut rencontrer de nombreux problèmes liés aux dimensions matrice/vecteur qui ne correspondent pas. En gardant un oeil sur les dimensions des différentes quantités matricielles, on peut s'affranchir d'un bon nombre de bugs\n",
    "\n",
    "**Exercice** Afficher les valeurs suivantes:\n",
    "    - m_train (taille de l'échantillon train)\n",
    "    - m_test (taille de l'échantillon test)\n",
    "    - num_px (nombre de pixel d'un côté du carré)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(X_train.shape))\n",
    "print (\"train_set_y shape: \" + str(Y_train.shape))\n",
    "print (\"test_set_x shape: \" + str(X_test.shape))\n",
    "print (\"test_set_y shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ouput**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td>**m_train**</td>\n",
    "    <td> 128 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**m_test**</td>\n",
    "    <td> 32 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**num_px**</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par soucis de clareté, il faut maintenant redimensionner les images de taille (num_px,num_px,3) en un array de dimension (num_px $*$ num_px $*$ 3, 1). Nos echantillons test et train seront donc des np.array où chaque colonne représente une image *plate*.\n",
    "\n",
    "**Exercice :** Transformer les échantillons test et train en np.array de dimensions (num_px $*$ num_px $*$ 3, 1).\n",
    "*Indication :* Pour transformer une matrice de dimensions ($n_1$,...,$n_p$) en matrice $(\\Pi n_i,n_1)$, on peut utiliser :\n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T est la transposée de X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(X_train_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(Y_train.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(X_test_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(Y_test.shape))\n",
    "print (\"sanity check after reshaping: \" + str(X_train_flatten[0:5,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**: \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>**train_set_x_flatten shape**</td>\n",
    "    <td> (12288, 128)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**train_set_y shape**</td>\n",
    "    <td>(1, 128)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**test_set_x_flatten shape**</td>\n",
    "    <td>(12288, 32)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**test_set_y shape**</td>\n",
    "    <td>(1, 32)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>**sanity check after reshaping**</td>\n",
    "  <td>[69 103 203  76 110]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une image est déterminée par les coordonnées RGB de chaque pixel qui définissent la couleur du pixel en question. Ainsi un pixel est un vecteur de dimension 3 contenant des valeurs entre 0 et 255.\n",
    "\n",
    "Il est commun en machine learning de normaliser les datasets (moyenne 0, variance 1). Pour les images il est plus simple (et cela fonctionne aussi très bien) de juste diviser par la valeur max de chaque coordonnée.\n",
    "\n",
    "**Warning**\n",
    "Durant l'entrainement, on est amené à multiplier les poids et additioner le biais des différents inputs afin de juger de l'activation et ensuite on rétropropage le gradient. Il est EXTREMEMENT important que toutes les features à l'entrée d'un neurones aient le même ordre de grandeur pour éviter que le gradient ne prenne des valeurs extrêmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Architecture générale ##\n",
    "\n",
    "On va maintenant construire un algorithme simple pour faire la distinction entre les deux catégorie de fleurs. On va construire une régression logistique en prenant le point de vue réseau de neurones. \n",
    "\n",
    "\n",
    "**Expressions mathématiques**:\n",
    "\n",
    "Pour un exemple $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = \\sigma(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "La fonction coût est obtenue comme la somme des $\\mathcal{L}$ de chaque exemple:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Les étapes**:\n",
    "Dans cet exercice, on va s'attaquer aux étapes suivantes:\n",
    "    - Initialiser les paramètres du modèle\n",
    "    - Apprendre les paramètres du modèle en minimisant $J$\n",
    "    - Utiliser les paramètres pour faire les prédictions\n",
    "    - Analyser les résultats et conclure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Construire les différentes briques de notre algorithme ##\n",
    "\n",
    "Les étapes principales pour construire un réseau de neurones sont les suivantes:\n",
    "1. Définir l'architecture du modèle (connections, taille de l'input, fonctions d'activations, etc...)\n",
    "2. Initialiser les paramètres du modèle\n",
    "3. Boucle :\n",
    "    - Calculer le coût (forward propagation)\n",
    "    - Calculer le gradient associé (backward propagation)\n",
    "    - Mettre à jour les paramètres (descente de gradient)\n",
    "\n",
    "En règle générale, on travaille séparèment sur les 3 points puis les intègre dans une fonction `model()`.\n",
    "\n",
    "### 4.1 - Fonctions utiles\n",
    "\n",
    "**Exercise**: Implémenter la fonction `sigmoid()` qui est notamment utilisé pour calculer $\\sigma( w^T x + b)$, la probabilité d'être dans la classe 1 (snowdrop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print (\"sigmoid(9.2) = \" + str(sigmoid(9.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**: \n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**sigmoid(0)**</td>\n",
    "    <td> 0.5</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**sigmoid(9.2)**</td>\n",
    "    <td> 0.999898970806 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Initialisation\n",
    "\n",
    "**Exercise:** Implémenter l'initialisation des paramètres (*cf* `np.zeros()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**: \n",
    "\n",
    "<table style=\"width:15%\">\n",
    "    <tr>\n",
    "        <td>  ** w **  </td>\n",
    "        <td> [[ 0.]\n",
    " [ 0.]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** b **  </td>\n",
    "        <td> 0 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Pour les images, w sera de taille (num_px $\\times$ num_px $\\times$ 3, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward et Backward propagation\n",
    "\n",
    "Maintenant que les paramètres sont initialisés, on peut effectuer les propagations *\"forward\"* et *\"backward\"* afin d'ajuster les paramètres au cours de l'apprentissage.\n",
    "\n",
    "\n",
    "\n",
    "**Exercice:** Implémenter la fonction `propagate()` qui calcule le coût et son gradient.\n",
    "\n",
    "**Indications**:\n",
    "\n",
    "Forward Propagation:\n",
    "- On connait X\n",
    "- On calcule $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- On calcule ensuite: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Voici les deux formules que vous utiliserez : \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "        <td>  ** dw **  </td>\n",
    "        <td> [[ 0.99993216]\n",
    " [ 1.99980262]]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** db **  </td>\n",
    "        <td> 0.499935230625 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** cost **  </td>\n",
    "        <td> 6.000064773192205</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Optimization\n",
    "Jusqu'ici on a:\n",
    "- initialisé les paramètres\n",
    "- créé une fonction pour calculer la fonction coût et son gradient\n",
    "\n",
    "Il s'agit maintenant de mettre à jour les paramètres au moyen la descente du gradient.\n",
    "\n",
    "**Exercice:** Implémenter la fonction d'optimisation. Le but est d'apprendre $w$ et $b$ en minimisant la fonction $J$. \n",
    "\n",
    "**Rappel**: Pour un paramètre $\\theta$, on applique $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, où $\\alpha$ est le *learning rate*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "    <tr>\n",
    "       <td> **w** </td>\n",
    "       <td>[[ 0.1124579 ]\n",
    " [ 0.23106775]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **b** </td>\n",
    "       <td> 1.55930492484 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **dw** </td>\n",
    "       <td> [[ 0.90158428]\n",
    " [ 1.76250842]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **db** </td>\n",
    "       <td> 0.430462071679 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** La fonction ci-dessus renvoie en sortie les paramètres appris w et b. On est maintenant en mesure de les utiliser w et b pour prédire les labels pour un dataset X. Il faut donc implémenter la fonction `predict()`. Pour cela, procédons en deux étapes :\n",
    "1. Calculer $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "2. Convertir $\\hat{Y}$ en booléen (0 si $A \\leq 0.5$, $1$ sinon), stocker les prédictions dans un vecteur `Y_prediction`. Si vous le souhaitez, vous pouvez utiliser `if`/`else` dans une boucle `for` même s'il existe aussi un moyen de le faire en vectoriel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             **predictions**\n",
    "         </td>\n",
    "          <td>\n",
    "            [[ 1.  1.]]\n",
    "         </td>  \n",
    "   </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**Ce qu'il faut retenir:**\n",
    "    \n",
    "On a implémenté plusieurs fonctions :\n",
    "-  Initialiser(w,b)\n",
    "-  Optimiser la fonction coût pour apprendre les paramètres (w,b) optimaux en:\n",
    "    - calculant le coût et son gradient\n",
    "    - mettant à jour les paramètres en *descendant* le long du gradient\n",
    "- Utiliser (w,b) pour prédire la classe d'un échantillon X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Merge all functions into a model ##\n",
    "\n",
    "On va maintenant voir comment le model s'articule en rassemblant tous les blocks dans une même fonction..\n",
    "\n",
    "**Exercise:** Implémenter la fonction `model` en utilisant les notations suivantes :\n",
    "    - Y_prediction pour la prédiction sur le test\n",
    "    - Y_prediction_train pour la prédiction sur le train \n",
    "    - w, costs, grads pour les outputs de optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    \n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    \n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    \n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_train\n",
    "    Y_prediction_test\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
