{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification avec une couche\n",
    "\n",
    "\n",
    "\n",
    "**On va apprendre à :**\n",
    "- Implémenter une classification à 2 classes avec une seule couche cachée\n",
    "- Utiliser un neurone avec une activation non linéaire (tanh)\n",
    "- Calculer la cross entropy\n",
    "- Implémenter la propagation backward et forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(pred_func,X,Y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y[:,0], cmap=plt.cm.Spectral,edgecolors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(np.divide(1,1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rosace(d,n,npoints,p):\n",
    "    np.random.seed(42)\n",
    "    t = np.random.rand(npoints)*2*3.1415\n",
    "    t = np.sort(t)\n",
    "    r = 0.05 * np.random.randn(npoints) + 1\n",
    "    X = np.zeros((t.shape[0],2))\n",
    "    X[:,0] = np.multiply(np.multiply(r,np.cos(t*n/d)),np.sin(t))\n",
    "    X[:,1] = np.multiply(np.multiply(r,np.cos(t*n/d)),np.cos(t))\n",
    "    Y = np.zeros((npoints,1))\n",
    "    thresholds = np.arange(0,npoints,npoints//(n*4))\n",
    "    noise = np.random.binomial(1, p, npoints)\n",
    "    temp = 0\n",
    "    classes = np.array([1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1])\n",
    "    for i,s in enumerate(thresholds[:-1]):\n",
    "        Y[s:thresholds[i+1],0] = (classes[i]+noise[s:thresholds[i+1]])%2\n",
    "    Y[thresholds[-1]:,0] = (classes[-1]+noise[s:thresholds[i+1]])%2\n",
    "    return(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ci-dessous va créer le data set contenant 400 points dans un plan. La rosace obtenue n'est pas linéairement séparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = create_rosace(1,4,400,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetons un oeil sur le dataset, contenant deux classes : rouge (y=0) et bleu (y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=Y[:,0],cmap=plt.cm.Spectral,edgecolors='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous avez:\n",
    "    - une matrie X qui contient les coordonnées (x1, x2)\n",
    "    - u vecteur Y qui contient les labels (red:0, blue:1).\n",
    "\n",
    "On commence par jeter un oeil rapide sur la donnée.\n",
    "\n",
    "**Exercice**: Combien d'exemple on a? Quelle est la dimension de `X` et `Y`? \n",
    "\n",
    "**Indication**: Dimension d'un array? [(help)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "       \n",
    "<table style=\"width:20%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**shape of X**</td>\n",
    "    <td> (2, 400) </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**shape of Y**</td>\n",
    "    <td>(1, 400) </td> \n",
    "  </tr>\n",
    "      <tr>\n",
    "    <td>**m**</td>\n",
    "    <td> 400 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Simple régression logistique\n",
    "\n",
    "Avant de se lancer sur le réseau de neurones, voyons comment se débrouille la régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression classifier\n",
    "clf = sklearn.linear_model.LogisticRegressionCV();\n",
    "clf.fit(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary for logistic regression\n",
    "plot_decision_boundary(lambda x: clf.predict(x),X,Y)\n",
    "plt.title(\"Logistic Regression\")\n",
    "\n",
    "# Print accuracy\n",
    "LR_predictions = clf.predict(X)\n",
    "print ('Accuracy of logistic regression: %d ' % float((np.dot(Y.T,LR_predictions) + np.dot(1-Y.T,1-LR_predictions))/float(Y.T.size)*100) +\n",
    "       '% ' + \"(percentage of correctly labelled datapoints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**Accuracy**</td>\n",
    "    <td> 55% </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: Le dataset n'est pas linéairement séparable,  la régression logistique a choisit une frontière de décision linéaire la plus acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Neural Network model\n",
    "\n",
    "La régression logistique ne convient pas pour le data set, on va donc tester un réseau de neurones à une couche.\n",
    "\n",
    "**Voici le modèle**:\n",
    "<img src=\"images/classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "**On a donc les formules suivantes**:\n",
    "\n",
    "Pour un exemple $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{sinon } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Etant donné les prédictions faites sur tous les exemples, on peut alors calculer J :\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**Rappel**: La méthodologie générale pour construire un réseau de neurones est la suivante\n",
    "    1. Définir l'architecture du réseau, ( # de l'input,  # couches, # units par couches, etc)\n",
    "    2. Initialiser les paramètres du modèle\n",
    "    3. Boucle sur le nombre d'itération :\n",
    "        - Forward propagation\n",
    "        - Calcule du coût\n",
    "        - Backward propagation pour obtenir le gradient relatif à chaque paramètre\n",
    "        - Mise à jour des paramètres\n",
    "\n",
    "Généralement, on construit les fonctions pour faire les étapes 1 à 3 et ensuite on rassemble le tout dans une fonction `nn_model()`, par exemple.\n",
    "Une fois `nn_model()`  construit et les paramètres appris, on peut faire la prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Définition de l'architecture ####\n",
    "\n",
    "**Exercicee**: Définir les variables:\n",
    "    - n_x: taille de l' input layer\n",
    "    - n_h: taille de la  couche cachée (set this to 4) \n",
    "    - n_y: taille de l'output\n",
    "\n",
    "**Indication**: Utiliser les dimensions de X et Y pour définir n_x et n_y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(n_x, n_h, n_y) = layer_sizes(np.zeros((5,10)), np.zeros(2))\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**n_x**</td>\n",
    "    <td> 5 </td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>**n_h**</td>\n",
    "    <td> 4 </td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>**n_y**</td>\n",
    "    <td> 2 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Initialisation des paramètres du modèle####\n",
    "\n",
    "**Exercise**: Implémenter `initialize_parameters()`.\n",
    "\n",
    "**Instructions**:\n",
    "- S'assurer que les dimensions des paramètres sont corrects en se référant à la figure ci-dessous si besoin.\n",
    "- La matrice des poids sera initialisée avec des valeurs aléatoires\n",
    "    -  `np.random.randn(a,b) * 0.01\n",
    "- Le bias est initialisé à un vecteur nul \n",
    "    - Use: `np.zeros((a,b))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(2, 4, 1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[-0.00416758 -0.00056267]\n",
    " [-0.02136196  0.01640271]\n",
    " [-0.01793436 -0.00841747]\n",
    " [ 0.00502881 -0.01245288]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - La boucle d'apprentissage####\n",
    "\n",
    "**Question**: Implémenter `forward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "- S'aider des formules ci-dessus \n",
    "- Utiliser la fonction `sigmoid()` implémentée au début du notebook\n",
    "- S'aider de numpy pour `tanh()`\n",
    "- Les étapes sont:\n",
    "    1. Retrouver chaque paramètre dans le dictionnaire \"parameters\" (output de `initialize_parameters()`)\n",
    "    2. Implémenter la Forward Propagation. Calculer $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (contenant les prédictions de l'ensemble des examples).\n",
    "- Les valeurs requises pour la backpropagation sont stockées dans `cache`. Le `cache` sera donné en input de la fonction de backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    " \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    " \n",
    "    ### END CODE HERE ###\n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "A2, cache = forward_propagation(np.random.randn(2,10),initialize_parameters(2, 4, 1))\n",
    "\n",
    "# Note: we use the mean here just to make sure that your output matches ours. \n",
    "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> 0.005798984461314502 0.005796610352404685 -0.000261742368547134 0.49993456441152634\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a calculé $A^{[2]}$, qui contient $a^{[2](i)}$ pour chaque exemplen on peut calculer la fonction coût :\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
    "\n",
    "**Exercice**: Implémenter `compute_cost()` pour calculer $J$.\n",
    "\n",
    "**Instructions**:\n",
    "- Il ya plusieurs façons d'implémenter la cross-entropy. Comme indication, voici comment j'aurais codé\n",
    "$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
    "```python\n",
    "logprobs = np.multiply(np.log(A2),Y)\n",
    "cost = - np.sum(logprobs)                # pas de boucle for!\n",
    "```\n",
    "\n",
    "(on peut aussi utiliser `np.multiply()` et ensuite `np.sum()` ou directement `np.dot()`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "source": [
    "**Question**: Implementer la fonction `backward_propagation()` en vous aidant des équations ci-dessous.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/grad_summary.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "<!--\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n",
    "\n",
    "- $*$ est la multiplication terme à terme.\n",
    "- on utilise les notations suivantes:\n",
    "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
    "    \n",
    "!-->\n",
    "\n",
    "- Tips:\n",
    "    - Pour calculer dZ1 on doit calculer $g^{[1]'}(Z^{[1]})$. Comme $g^{[1]}(.)$ est tanh, si $a = g^{[1]}(z)$ alors $g^{[1]'}(z) = 1-a^2$. On peut calculer \n",
    "    $g^{[1]'}(Z^{[1]})$ en utilisant `(1 - np.power(A1, 2))`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Intégrer 4.1, 4.2 et 4.3 dans nn_model() ####\n",
    "\n",
    "**Question**: Intégrer dans `nn_model()`.\n",
    "\n",
    "**Instruction**: Le réseau de neurones doit utiliser les fonctions construites précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "   \n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        \n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        \n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        \n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Predictions\n",
    "\n",
    "**Question**: Utiliser le modèke appris pour construire predict().\n",
    "Utiliser la forward propagation pour calculer la prédiction\n",
    "\n",
    "**Rappel**: predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{si}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{sinon}\n",
    "    \\end{cases}$  \n",
    "\n",
    "Par exemple, si on veut définir les entrées d'un array X comme 0 ou 1 en fonction d'un seuil on peut utiliser: ```X_new = (X > seuil)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons comment se comporte le modèle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = nn_model(X.T, Y.T, n_h = 4, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>**Cost after iteration 9000**</td>\n",
    "    <td> 0.412498 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X.T)\n",
    "print ('Accuracy: %d' % float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.T.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td>**Accuracy**</td>\n",
    "    <td> 86% </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuracy est particulièrement élevée comparée à la régression logistique. Le modèle a pu apprendre le pattern de la rosace. En réalité, les réseaux de neurones sont capables d'apprendre les frontières de décisions avec des non-linéarités importantes (si tant est qu'on ait assez de données), à la différence de la régression logistique.\n",
    "\n",
    "Essayons d'augmenter la taille de la couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~2 minutes to run\n",
    "\n",
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Hidden Layer of size %d' % n_h)\n",
    "    parameters = nn_model(X.T, Y.T, n_h, num_iterations = 5000)\n",
    "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "    predictions = predict(parameters, X.T)\n",
    "    accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.T.size)*100)\n",
    "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation**:\n",
    "- Plus le modèle possède de neurones dans la couche, les plus gros modèles peuvent même complétement surapprendre.\n",
    "- La meilleure taille semble être autour de n_h=4. Les plus gros modèles tendent à chercher l'overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autres questions**:\n",
    "\n",
    "Quelques questions optionnelles : \n",
    "- Que se passe t'il si on change tanh par un sigmoid ou une ReLu?\n",
    "- Que se passe t'il lorsqu'on change le learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}